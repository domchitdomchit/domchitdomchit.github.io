{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_텍스트_전처리_2_정제정규화.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPYncfDK7E7pYvayAfYTOsN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doomchitdoomchit/domchitdomchit.github.io/blob/master/NLP/1_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EC%A0%84%EC%B2%98%EB%A6%AC_2_%EC%A0%95%EC%A0%9C%EC%A0%95%EA%B7%9C%ED%99%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg22Fo-bpkZ-"
      },
      "source": [
        "# 2. 정제와 정규화\n",
        ": 코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화라고 하며, 토큰화 작업 전, 후에는 텍스트 데이터를 용도에 맞게 정제 및 정규화하는 일이 항상 있음.\n",
        "  * 정제 : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\n",
        "  * 정규화 : 표현 방법이 다른 단어들을 통합시키서 같은 단어로 만들어 준다.\n",
        "\n",
        "정제 작업은 토큰화 작업에 방해가 되는 부분을 배제시키고 토큰화 작업을 수행하기 위해서 토큰화 작업보다 앞서 이루어지기도 하지만, 토큰화 작업 이후에도 여전히 남아 있는 노이즈들을 제거하기 위해 지속적으로 이루어지기도 한다.   \n",
        "사실 완벽한 정제 작업은 어려운 편이라서, 대부분의 경우 이 정도면 됐다.라는 일종의 합의점을 찾기도 한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHRstTy7q8JT"
      },
      "source": [
        "## 1.규칙에 기반한 표기가 다른 단어들의 통합\n",
        ": 필요에 따라 직접 코딩을 통해 정의할 수 있는 정규화 규칙의 예로서 같은 의미를 갖고 있음에도, 표기가 다른 단어들을 하나의 단어로 정규화하는 방법을 사용할 수 있음.\n",
        "\n",
        "예로 USA와 US는 같은 의미를 가지므로, 하나의 단어로 정규화해 볼 수 있습니다. 이허한 정규화를 거치게 되면, US를 찾아도 USA를 찾을 수 있음 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8qOvw1suGPY"
      },
      "source": [
        "## 2.대, 소문자 통합\n",
        ": 영어권에서 대, 소문자를 통합하는 것은 단어의 개수를 줄일 수 있는 또 다른 정규화 방법. 영어권에서 대문자는 문장의 맨 앞 등과 같은 특정한 상황에서만 쓰이고, 대부부은 소문자로 작성되기 때문에, 대소문자 통합 작업은 대부분 대문자를 소문자로 변환하는 소문자 변환 작업으로 이루어진다.\n",
        "\n",
        "예로는 검색엔진에서 사용자가 페라리 차량에 관심이 있어서 페라리를 검색해 본다고 하면, 엄밀히 말해서 사용자가 검색해야하는 페라리는 a Ferrari car임. 하지만 검색엔진에서 소문자 변환을 적용했을 것이기 때문에 소문자로 쳐도 원하는 결과를 얻음.\n",
        "\n",
        "물론 무조건적인 소문자 변환은 안된. 왜냐하면 나라이름, 사람이름 등 고유명사는 대문자로 유지되어야 한다.\n",
        "\n",
        "이렇게 모든 토큰을 소문자 변환을 해서 문제가 생기면 일부만 소문자 변환 하는 방법이 있음.(문장 맨 앞의 대문자만 소문자로 바꾸고 다른 곳의 대문자는 대문자 상태로 놔둔는 것)\n",
        "\n",
        "사실 이러한 작업은 더 많은 변수를 사용해서 소문자 변환을 언제 사용할지 결정하는 머신러닝 시쿼스 모델로 더 정확하게 진행할 수 있음. 하지만 올바른 대문자 단어를 얻고 싶은 상황에서 훈련에 사용하는 코퍼스가 올바른 대소문자를 구분에서 사용하는 사람이어야 한다. 이러한 중대한 어려움에 예외사항에 크게 고려하지 않고 그냥 모든 코퍼스를 소문자로 변환하는 것이 좀 더 실용적 해결책이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4j1aaB2vDSo"
      },
      "source": [
        "## 3.불필요한 단어의 제거\n",
        ": 정제 작업에서 제거해야하는 노이즈 데이터는 자연어가 아니면서 아무 의미도 갖지 않는 글자들(특수 문자)을 의미하기도 하지만, 분석하고자 하는 목적에 맞지 않는 불필요한 단어들을 노이즈 데이터라고 함.\n",
        "\n",
        "불필요 단어들을 제거하는 방법으로는 불용어 제거와 등장 빈도가 적은 단어, 길이가 짧은 단어들을 제거하는 방법이 있다.\n",
        "\n",
        "  1. 등장 빈도가 적은 단어   \n",
        "  : 때론 텍스트 데이터에서 너무 적게 등장해서 자연어 처리에 도움이 되지 않는 단어들이 존재. 예로 스팸메일 구분모델에서 1000000개의 메일 데이터에서 총합이 5번 밖에 등장하지 않은 단어가 있다면 이 단어는 직관적으로 분류에 거의 도움이 되지 않음을 알 수 있다.\n",
        "\n",
        "  2. 길이가 짧은 단어   \n",
        "  : 영어권에서는 길이가 짧은 단어를 삭제하는 것만으로도 어느정도 자연어 처리에서 크게 의미없는 단어를 제거하는 효과를 볼 수 있다고 알려짐. 즉, 길이가 짧은 단어의 대부분이 불용어에 해당됨. 하지만 한국에서는 길이가 짧은 단어라고 삭제하는 이런 방법은 크게 유효하지 않음\n",
        "\n",
        "  영어 단어의 평균 길이는 6 ~ 7이며, 한국어 단어의 평균길이는 2 ~ 3 정도로 추정됨. 정확하다고 말할 수는 없지만 확실한 것은 영어가 한국어보다 길다.\n",
        "\n",
        "  한국어 간어는 한자어가 많고, 한 글자로 의미를 갖는 경우가 많다.\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npytLcnvy9fO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1f7971ef-ffe1-41f9-e552-7ff15afbd3dc"
      },
      "source": [
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "print(shortword.sub('', text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " was wondering anyone out there could enlighten this car.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpR7ud-bzFDL"
      },
      "source": [
        "## 4.정규 표현식\n",
        ": 얻어낸 코퍼스에서 노이즈 데이터의 특징을 잡아낼 수 있다면, 정규 표현식을 통해서 이를 제거할 수 있는 경우가 많음. 예로, HTML로 크롤링해온 코퍼스라면, 문서 여기저기에 HTML 태그가 있음. 뉴스였다면, 시간이 적혀있을 수 있음. 정규 표현식은 이러한 코퍼스 내에 계속해서 등장하는 글자들을 규칙에 기반하여 한 번에 제거하는 방식으로 매우 유용 "
      ]
    }
  ]
}